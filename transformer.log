=== Logging started at Thu Jan  2 20:52:44 2025
===
entering TransformerConfig constructor
exiting TransformerConfig constructor
Initializing transformer with configuration:
- Hidden size: 768
- Attention heads: 12
- Layers: 6
- Using Flash Attention: false
- Using RoPE: true
- Using Sliding Window: true
entering Transformer constructor
entering TransformerLayer constructor
entering MultiHeadAttention constructor
Initialization scales:
q_scale: 0.0360844
kv_scale: 0.049029
out_scale: 0.0360844
query_proj stats after init:
  Shape: 768x768
  Range: [-0.0360842, 0.0360844]
key_proj stats after init:
  Shape: 768x768
  Range: [-0.0490288, 0.049029]
value_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.0490288]
output_proj stats after init:
  Shape: 768x768
  Range: [-0.0360843, 0.0360843]
exiting MultiHeadAttention constructor
FeedForward dimensions:
w1: 768x3072
w2: 3072x768
exiting TransformerLayer constructor
entering TransformerLayer constructor
entering MultiHeadAttention constructor
Initialization scales:
q_scale: 0.0360844
kv_scale: 0.049029
out_scale: 0.0360844
query_proj stats after init:
  Shape: 768x768
  Range: [-0.0360843, 0.0360844]
key_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.049029]
value_proj stats after init:
  Shape: 768x768
  Range: [-0.0490289, 0.0490289]
output_proj stats after init:
  Shape: 768x768
  Range: [-0.0360841, 0.0360843]
exiting MultiHeadAttention constructor
FeedForward dimensions:
w1: 768x3072
w2: 3072x768
exiting TransformerLayer constructor
entering TransformerLayer constructor
entering MultiHeadAttention constructor
Initialization scales:
q_scale: 0.0360844
kv_scale: 0.049029
out_scale: 0.0360844
query_proj stats after init:
  Shape: 768x768
  Range: [-0.0360843, 0.0360842]
key_proj stats after init:
  Shape: 768x768
  Range: [-0.0490289, 0.0490286]
value_proj stats after init:
  Shape: 768x768
  Range: [-0.0490286, 0.049029]
output_proj stats after init:
  Shape: 768x768
  Range: [-0.0360843, 0.0360843]
exiting MultiHeadAttention constructor
FeedForward dimensions:
w1: 768x3072
w2: 3072x768
exiting TransformerLayer constructor
entering TransformerLayer constructor
entering MultiHeadAttention constructor
Initialization scales:
q_scale: 0.0360844
kv_scale: 0.049029
out_scale: 0.0360844
query_proj stats after init:
  Shape: 768x768
  Range: [-0.0360842, 0.0360844]
key_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.0490286]
value_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.0490288]
output_proj stats after init:
  Shape: 768x768
  Range: [-0.0360842, 0.0360841]
exiting MultiHeadAttention constructor
FeedForward dimensions:
w1: 768x3072
w2: 3072x768
exiting TransformerLayer constructor
entering TransformerLayer constructor
entering MultiHeadAttention constructor
Initialization scales:
q_scale: 0.0360844
kv_scale: 0.049029
out_scale: 0.0360844
query_proj stats after init:
  Shape: 768x768
  Range: [-0.0360843, 0.0360844]
key_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.0490289]
value_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.049029]
output_proj stats after init:
  Shape: 768x768
  Range: [-0.0360844, 0.0360844]
exiting MultiHeadAttention constructor
FeedForward dimensions:
w1: 768x3072
w2: 3072x768
exiting TransformerLayer constructor
entering TransformerLayer constructor
entering MultiHeadAttention constructor
Initialization scales:
q_scale: 0.0360844
kv_scale: 0.049029
out_scale: 0.0360844
query_proj stats after init:
  Shape: 768x768
  Range: [-0.0360843, 0.0360842]
key_proj stats after init:
  Shape: 768x768
  Range: [-0.0490289, 0.049029]
value_proj stats after init:
  Shape: 768x768
  Range: [-0.049029, 0.0490289]
output_proj stats after init:
  Shape: 768x768
  Range: [-0.0360841, 0.0360844]
exiting MultiHeadAttention constructor
FeedForward dimensions:
w1: 768x3072
w2: 3072x768
exiting TransformerLayer constructor
entering TransformerLayer::convert_to_fp16
exiting TransformerLayer::convert_to_fp16
entering TransformerLayer::convert_to_fp16
exiting TransformerLayer::convert_to_fp16
entering TransformerLayer::convert_to_fp16
exiting TransformerLayer::convert_to_fp16
entering TransformerLayer::convert_to_fp16
exiting TransformerLayer::convert_to_fp16
entering TransformerLayer::convert_to_fp16
exiting TransformerLayer::convert_to_fp16
entering TransformerLayer::convert_to_fp16
exiting TransformerLayer::convert_to_fp16
exiting Transformer constructor
LM Head initialization:
Creating projection matrix: [50000 Ã— 768]

Verifying vocabulary mappings:

=== Special Tokens ===
PAD token: 0 <-> <pad>
UNK token: 1 <-> <unk>
BOS token: 2 <-> <bos>
EOS token: 3 <-> <eos>

=== Full Vocabulary ===
Total size: 510 tokens

entering create_training_data

=== Logging stopped at Thu Jan  2 20:52:59 2025
===
