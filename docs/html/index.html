<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Transformer CPP: A decoder-style Transformer in C++</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Transformer CPP
   </div>
   <div id="projectbrief">A C++/CUDA implementation of a Transformer model</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">A decoder-style <a class="el" href="classTransformer.html" title="Main Transformer model implementing the standard Transformer architecture.">Transformer</a> in C++ </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> A pure C++ implementation of a decoder-only transformer model with CUDA support. It is based on the paper "Attention is All You Need" by Vaswani et al and has been trained on an example dataset found in the <code>data</code> directory called 'training_pairs.txt'. It performs a single token prediction for each input.</p>
<h1><a class="anchor" id="autotoc_md1"></a>
Transformer Implementation Features</h1>
<h1><a class="anchor" id="autotoc_md2"></a>
Core Attention Mechanisms</h1>
<ul>
<li>Standard Multi-Head Attention</li>
<li>Grouped Query Attention (GQA)</li>
<li>Flash Attention optimization</li>
<li>Rotary Position Embeddings (RoPE)</li>
<li>Sliding Window Attention</li>
<li>Key-Value Cache support</li>
</ul>
<h1><a class="anchor" id="autotoc_md3"></a>
Architecture Components</h1>
<ul>
<li>Layer Normalization</li>
<li>Feed Forward Networks</li>
<li><a class="el" href="classDropout.html" title="Implements dropout regularization for neural networks.">Dropout</a> layers</li>
<li>Residual connections</li>
<li>Language Model Head</li>
<li><a class="el" href="classTokenizer.html" title="Text tokenizer for converting between text and token sequences.">Tokenizer</a> with vocabulary management</li>
</ul>
<h1><a class="anchor" id="autotoc_md4"></a>
Training Features</h1>
<ul>
<li>Batch processing</li>
<li>Beam Search</li>
<li>Dynamic learning rate adjustment</li>
<li>Gradient clipping</li>
<li>Loss computation and backpropagation</li>
<li>Training/Evaluation modes</li>
<li>Gradient checkpointing</li>
<li>Performance metrics tracking</li>
</ul>
<h1><a class="anchor" id="autotoc_md5"></a>
Optimization Features</h1>
<ul>
<li>CUDA support for GPU acceleration</li>
<li>OpenMP parallelisation</li>
<li>Half-precision (FP16) support</li>
<li>Memory pooling</li>
<li>Gradient accumulation</li>
<li><a class="el" href="classSAM.html">SAM</a> (Sharpness-Aware Minimization) optimizer</li>
</ul>
<h1><a class="anchor" id="autotoc_md6"></a>
Advanced Features</h1>
<ul>
<li>Quantization-Aware Training</li>
<li>Adaptive cache replacement policies</li>
<li>Token embedding with positional encoding</li>
<li>Advanced attention mechanisms (block-sparse)</li>
<li>Configurable model architecture</li>
</ul>
<h1><a class="anchor" id="autotoc_md7"></a>
Utility Features</h1>
<ul>
<li>JSON configuration loading</li>
<li>Model checkpointing and saving</li>
<li>Performance metrics logging</li>
<li>Validation data evaluation</li>
<li>Token prediction and probability calculation</li>
<li>Text preprocessing and tokenization</li>
</ul>
<h1><a class="anchor" id="autotoc_md8"></a>
Memory Management</h1>
<ul>
<li>Memory pooling</li>
<li>Cache management</li>
<li>Gradient checkpointing</li>
<li>Efficient matrix operations</li>
</ul>
<h1><a class="anchor" id="autotoc_md9"></a>
Development Features</h1>
<ul>
<li>Comprehensive logging</li>
<li>Error handling</li>
<li>Configuration validation</li>
<li>Performance profiling</li>
<li>Debug output options</li>
</ul>
<h1><a class="anchor" id="autotoc_md10"></a>
Dependencies</h1>
<ul>
<li>OpenMP: <a href="https://github.com/OpenMP/openmp-api">https://github.com/OpenMP/openmp-api</a></li>
<li>CUDA: <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></li>
<li>nlohmann/json: <a href="https://github.com/nlohmann/json">https://github.com/nlohmann/json</a></li>
</ul>
<h1><a class="anchor" id="autotoc_md11"></a>
Building</h1>
<p>To build the project, you can use the following commands:</p>
<div class="fragment"><div class="line">mkdir build</div>
<div class="line">cd build</div>
<div class="line">cmake ..</div>
<div class="line">make</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md12"></a>
Training the model</h1>
<p>After building the project, running <code><a class="el" href="main_8cpp.html">main.cpp</a></code> will train the model and save the model hyperparamters to whatever directory is specified in the <code>config/transformer_config.json</code> file. To execute the training on the sample dataset, run the following command from the build directory:</p>
<div class="fragment"><div class="line">./transformer</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md13"></a>
Logging</h1>
<p>The logging is done to a file called <code>transformer.log</code> in the <code>build</code> directory.</p>
<h1><a class="anchor" id="autotoc_md14"></a>
Configuration</h1>
<p>The configuration is done in the <code>config/transformer_config.json</code> file.</p>
<h1><a class="anchor" id="autotoc_md15"></a>
Limitations</h1>
<ul>
<li>The model training is performed on a very small dataset, so its predictions are certainly sub-optimal, given its constraints.</li>
<li>It only works on a format that follows the training data i.e I like to cook in the |kitchen (where <code>|</code> is the delimiter). </li>
</ul>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
