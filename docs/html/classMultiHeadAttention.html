<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Transformer CPP: MultiHeadAttention Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Transformer CPP
   </div>
   <div id="projectbrief">A C++/CUDA implementation of a Transformer model</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a> &#124;
<a href="#friends">Friends</a> &#124;
<a href="classMultiHeadAttention-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">MultiHeadAttention Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Implementation of Multi-Head Attention mechanism with various optimizations.  
 <a href="classMultiHeadAttention.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="attention_8hpp_source.html">attention.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for MultiHeadAttention:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classMultiHeadAttention.png" usemap="#MultiHeadAttention_map" alt=""/>
  <map id="MultiHeadAttention_map" name="MultiHeadAttention_map">
<area href="classSlidingWindowAttention.html" alt="SlidingWindowAttention" shape="rect" coords="0,56,143,80"/>
<area href="classSparseAttention.html" alt="SparseAttention" shape="rect" coords="153,56,296,80"/>
  </map>
</div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a8e0b7839939a2076d6c92a8a59443364" id="r_a8e0b7839939a2076d6c92a8a59443364"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a8e0b7839939a2076d6c92a8a59443364">~MultiHeadAttention</a> ()=default</td></tr>
<tr class="separator:a8e0b7839939a2076d6c92a8a59443364"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afd7c283ed2457ad7304567c897b3d877" id="r_afd7c283ed2457ad7304567c897b3d877"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#afd7c283ed2457ad7304567c897b3d877">MultiHeadAttention</a> ()=default</td></tr>
<tr class="separator:afd7c283ed2457ad7304567c897b3d877"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5bb39f2850f5824fd23df006b4693299" id="r_a5bb39f2850f5824fd23df006b4693299"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a5bb39f2850f5824fd23df006b4693299">MultiHeadAttention</a> (size_t hidden_size_, size_t num_heads_, size_t head_dim_, float dropout_prob_, bool use_flash_, bool use_rope_, bool use_sliding_window_, size_t window_size_, bool use_gqa_, size_t num_kv_heads_, size_t max_seq_length_, bool use_fp16)</td></tr>
<tr class="memdesc:a5bb39f2850f5824fd23df006b4693299"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs a multi-head attention layer with the specified parameters.  <br /></td></tr>
<tr class="separator:a5bb39f2850f5824fd23df006b4693299"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9c5b1b6ba053f3891ddde42588fb8e7e" id="r_a9c5b1b6ba053f3891ddde42588fb8e7e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a9c5b1b6ba053f3891ddde42588fb8e7e">forward</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;x, const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;mask, const std::optional&lt; <a class="el" href="structKVCache.html">KVCache</a> &gt; &amp;kv_cache=std::nullopt)</td></tr>
<tr class="memdesc:a9c5b1b6ba053f3891ddde42588fb8e7e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs the forward pass of the attention mechanism.  <br /></td></tr>
<tr class="separator:a9c5b1b6ba053f3891ddde42588fb8e7e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d5c79966001a4f69b95c8fdcad85e2b" id="r_a0d5c79966001a4f69b95c8fdcad85e2b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a0d5c79966001a4f69b95c8fdcad85e2b">backward</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;grad_output, const <a class="el" href="classMatrix.html">Matrix</a> &amp;input, const <a class="el" href="classMatrix.html">Matrix</a> &amp;target_distribution)</td></tr>
<tr class="memdesc:a0d5c79966001a4f69b95c8fdcad85e2b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs the backward pass to compute gradients.  <br /></td></tr>
<tr class="separator:a0d5c79966001a4f69b95c8fdcad85e2b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a294080c43c3b337d7525e7b7e380d89c" id="r_a294080c43c3b337d7525e7b7e380d89c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a294080c43c3b337d7525e7b7e380d89c">backward_cuda</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;grad, const <a class="el" href="classMatrix.html">Matrix</a> &amp;input) const</td></tr>
<tr class="memdesc:a294080c43c3b337d7525e7b7e380d89c"><td class="mdescLeft">&#160;</td><td class="mdescRight">CUDA-accelerated version of the backward pass.  <br /></td></tr>
<tr class="separator:a294080c43c3b337d7525e7b7e380d89c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0386f6537c985151921e37a2008d6d59" id="r_a0386f6537c985151921e37a2008d6d59"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a0386f6537c985151921e37a2008d6d59">save</a> (std::ostream &amp;os) const</td></tr>
<tr class="memdesc:a0386f6537c985151921e37a2008d6d59"><td class="mdescLeft">&#160;</td><td class="mdescRight">Saves the attention layer parameters to a stream.  <br /></td></tr>
<tr class="separator:a0386f6537c985151921e37a2008d6d59"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8c6f562cadc8d96c6d813903ab68eba0" id="r_a8c6f562cadc8d96c6d813903ab68eba0"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::reference_wrapper&lt; <a class="el" href="classMatrix.html">Matrix</a> &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a8c6f562cadc8d96c6d813903ab68eba0">get_weights</a> ()</td></tr>
<tr class="memdesc:a8c6f562cadc8d96c6d813903ab68eba0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets references to all trainable weight matrices.  <br /></td></tr>
<tr class="separator:a8c6f562cadc8d96c6d813903ab68eba0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acc1115481c87ae82ef5f5dc1240abd6c" id="r_acc1115481c87ae82ef5f5dc1240abd6c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#acc1115481c87ae82ef5f5dc1240abd6c">getQueryBias</a> ()</td></tr>
<tr class="separator:acc1115481c87ae82ef5f5dc1240abd6c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8f6731eea02865fe7875c2bc51f8aaab" id="r_a8f6731eea02865fe7875c2bc51f8aaab"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a8f6731eea02865fe7875c2bc51f8aaab">getKeyBias</a> ()</td></tr>
<tr class="separator:a8f6731eea02865fe7875c2bc51f8aaab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac88561cc62d2f36f06ca0425a379d635" id="r_ac88561cc62d2f36f06ca0425a379d635"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ac88561cc62d2f36f06ca0425a379d635">getValueBias</a> ()</td></tr>
<tr class="separator:ac88561cc62d2f36f06ca0425a379d635"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab8ac3041965fd7cd55c5c6baddedec10" id="r_ab8ac3041965fd7cd55c5c6baddedec10"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ab8ac3041965fd7cd55c5c6baddedec10">getOutputBias</a> ()</td></tr>
<tr class="separator:ab8ac3041965fd7cd55c5c6baddedec10"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5c83c47bf2bf22760110edeb4375ac8e" id="r_a5c83c47bf2bf22760110edeb4375ac8e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a5c83c47bf2bf22760110edeb4375ac8e">MultiHeadAttention</a> (const <a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &amp;other)</td></tr>
<tr class="separator:a5c83c47bf2bf22760110edeb4375ac8e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1af65b07493c526796f14b9d7599f364" id="r_a1af65b07493c526796f14b9d7599f364"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a1af65b07493c526796f14b9d7599f364">operator=</a> (const <a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &amp;other)</td></tr>
<tr class="separator:a1af65b07493c526796f14b9d7599f364"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19414c0bde6474025e97ab357da7819e" id="r_a19414c0bde6474025e97ab357da7819e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a19414c0bde6474025e97ab357da7819e">parameters</a> ()</td></tr>
<tr class="separator:a19414c0bde6474025e97ab357da7819e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91beae69ea948d75a451e65b568ed13d" id="r_a91beae69ea948d75a451e65b568ed13d"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a91beae69ea948d75a451e65b568ed13d">parameter_gradients</a> () const</td></tr>
<tr class="separator:a91beae69ea948d75a451e65b568ed13d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36b374cd10b12a404825b10ca5371096" id="r_a36b374cd10b12a404825b10ca5371096"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a36b374cd10b12a404825b10ca5371096">compute_attention_scores</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;Q, const <a class="el" href="classMatrix.html">Matrix</a> &amp;K)</td></tr>
<tr class="memdesc:a36b374cd10b12a404825b10ca5371096"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes attention scores between query and key matrices.  <br /></td></tr>
<tr class="separator:a36b374cd10b12a404825b10ca5371096"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-static-methods" name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:acdb722e2273e30fee53d5dc11d60e956" id="r_acdb722e2273e30fee53d5dc11d60e956"><td class="memItemLeft" align="right" valign="top">static std::unique_ptr&lt; <a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#acdb722e2273e30fee53d5dc11d60e956">load</a> (std::istream &amp;is, const <a class="el" href="classTransformerConfig.html">TransformerConfig</a> &amp;config)</td></tr>
<tr class="memdesc:acdb722e2273e30fee53d5dc11d60e956"><td class="mdescLeft">&#160;</td><td class="mdescRight">Loads attention layer parameters from a stream.  <br /></td></tr>
<tr class="separator:acdb722e2273e30fee53d5dc11d60e956"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:a15a9a52d7adb5b6c4fe5cddb273acff6" id="r_a15a9a52d7adb5b6c4fe5cddb273acff6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classVector.html">Vector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a15a9a52d7adb5b6c4fe5cddb273acff6">apply_rope</a> (const <a class="el" href="classVector.html">Vector</a> &amp;x, size_t position) const</td></tr>
<tr class="separator:a15a9a52d7adb5b6c4fe5cddb273acff6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ae83b92dbfb2da91f2a158d17c701ff" id="r_a2ae83b92dbfb2da91f2a158d17c701ff"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a2ae83b92dbfb2da91f2a158d17c701ff">flash_attention</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;Q, const <a class="el" href="classMatrix.html">Matrix</a> &amp;K, const <a class="el" href="classMatrix.html">Matrix</a> &amp;V, const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;mask) const</td></tr>
<tr class="separator:a2ae83b92dbfb2da91f2a158d17c701ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa63f2eb55c928afcbaabe2bed9b5fe9f" id="r_aa63f2eb55c928afcbaabe2bed9b5fe9f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aa63f2eb55c928afcbaabe2bed9b5fe9f">standard_attention</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;Q, const <a class="el" href="classMatrix.html">Matrix</a> &amp;K, const <a class="el" href="classMatrix.html">Matrix</a> &amp;V, const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;mask)</td></tr>
<tr class="separator:aa63f2eb55c928afcbaabe2bed9b5fe9f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad58475dba6a9acf0f788a069807ca678" id="r_ad58475dba6a9acf0f788a069807ca678"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ad58475dba6a9acf0f788a069807ca678">reshape_for_attention</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;x, size_t batch_size, size_t <a class="el" href="classMultiHeadAttention.html#a34cbd06de38900793f5c452507b032fa">num_heads</a>, size_t seq_len, size_t head_size) const</td></tr>
<tr class="separator:ad58475dba6a9acf0f788a069807ca678"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff569533c7e7f11a7fa4b6ba1662efc5" id="r_aff569533c7e7f11a7fa4b6ba1662efc5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aff569533c7e7f11a7fa4b6ba1662efc5">reshape_from_attention</a> (const <a class="el" href="classTensor.html">Tensor</a> &amp;x, size_t seq_len, size_t <a class="el" href="classMultiHeadAttention.html#afa3559e2a63108418d209926f89d7f13">hidden_size</a>) const</td></tr>
<tr class="separator:aff569533c7e7f11a7fa4b6ba1662efc5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0df29ad720ff58e8ffd73d3fcb2e6b26" id="r_a0df29ad720ff58e8ffd73d3fcb2e6b26"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a0df29ad720ff58e8ffd73d3fcb2e6b26">compute_attention</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;Q, const <a class="el" href="classMatrix.html">Matrix</a> &amp;K, const <a class="el" href="classMatrix.html">Matrix</a> &amp;V, const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;mask, size_t batch_size, size_t <a class="el" href="classMultiHeadAttention.html#a34cbd06de38900793f5c452507b032fa">num_heads</a>, size_t seq_len, size_t head_size)</td></tr>
<tr class="separator:a0df29ad720ff58e8ffd73d3fcb2e6b26"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a595e7485ec3665e5dae0b13e820a529b" id="r_a595e7485ec3665e5dae0b13e820a529b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a595e7485ec3665e5dae0b13e820a529b">validate_dimensions</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;grad_output, const <a class="el" href="classMatrix.html">Matrix</a> &amp;input, const <a class="el" href="classMatrix.html">Matrix</a> &amp;target_dist) const</td></tr>
<tr class="separator:a595e7485ec3665e5dae0b13e820a529b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa153e5d1204e9e40db741b979b8da86d" id="r_aa153e5d1204e9e40db741b979b8da86d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aa153e5d1204e9e40db741b979b8da86d">compute_query_gradients</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;grad_output, const <a class="el" href="classMatrix.html">Matrix</a> &amp;input) const</td></tr>
<tr class="separator:aa153e5d1204e9e40db741b979b8da86d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2af0984f2e552f9bbad0eb9a4f33e1dc" id="r_a2af0984f2e552f9bbad0eb9a4f33e1dc"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a2af0984f2e552f9bbad0eb9a4f33e1dc">compute_key_gradients</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;grad_output, const <a class="el" href="classMatrix.html">Matrix</a> &amp;input) const</td></tr>
<tr class="separator:a2af0984f2e552f9bbad0eb9a4f33e1dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afb26ebaa99dac3ec111eeeb60baf96cf" id="r_afb26ebaa99dac3ec111eeeb60baf96cf"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#afb26ebaa99dac3ec111eeeb60baf96cf">compute_value_gradients</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;grad_output, const <a class="el" href="classMatrix.html">Matrix</a> &amp;input) const</td></tr>
<tr class="separator:afb26ebaa99dac3ec111eeeb60baf96cf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6e2075c78c6a20f11ea661d31ecffa32" id="r_a6e2075c78c6a20f11ea661d31ecffa32"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a6e2075c78c6a20f11ea661d31ecffa32">combine_gradients</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;dQ, const <a class="el" href="classMatrix.html">Matrix</a> &amp;dK, const <a class="el" href="classMatrix.html">Matrix</a> &amp;dV) const</td></tr>
<tr class="separator:a6e2075c78c6a20f11ea661d31ecffa32"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab19f508223deaa017a7f924f0946d25a" id="r_ab19f508223deaa017a7f924f0946d25a"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ab19f508223deaa017a7f924f0946d25a">compute_attention</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;Q, const <a class="el" href="classMatrix.html">Matrix</a> &amp;K, const <a class="el" href="classMatrix.html">Matrix</a> &amp;V, const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;mask)</td></tr>
<tr class="separator:ab19f508223deaa017a7f924f0946d25a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2acdc3607094de7bd5e121432084947" id="r_ad2acdc3607094de7bd5e121432084947"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ad2acdc3607094de7bd5e121432084947">safe_matmul</a> (const <a class="el" href="classMatrix.html">Matrix</a> &amp;A, const <a class="el" href="classMatrix.html">Matrix</a> &amp;B)</td></tr>
<tr class="separator:ad2acdc3607094de7bd5e121432084947"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3ccbdfe184867bfb5ba90cd103bd2eb0" id="r_a3ccbdfe184867bfb5ba90cd103bd2eb0"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a3ccbdfe184867bfb5ba90cd103bd2eb0">apply_mask</a> (<a class="el" href="classMatrix.html">Matrix</a> &amp;scores, const <a class="el" href="classMatrix.html">Matrix</a> &amp;mask) const</td></tr>
<tr class="separator:a3ccbdfe184867bfb5ba90cd103bd2eb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a666bd13dfa5833931c2b3176ebf3d2f8" id="r_a666bd13dfa5833931c2b3176ebf3d2f8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a666bd13dfa5833931c2b3176ebf3d2f8">apply_stable_softmax</a> (<a class="el" href="classMatrix.html">Matrix</a> &amp;x) const</td></tr>
<tr class="separator:a666bd13dfa5833931c2b3176ebf3d2f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3bb386388f773c8b72eb2af62c66219c" id="r_a3bb386388f773c8b72eb2af62c66219c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a3bb386388f773c8b72eb2af62c66219c">apply_mask</a> (<a class="el" href="classTensor.html">Tensor</a> &amp;scores, const <a class="el" href="classMatrix.html">Matrix</a> &amp;mask) const</td></tr>
<tr class="separator:a3bb386388f773c8b72eb2af62c66219c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a615af977a42e414b297cc38aaa583a96" id="r_a615af977a42e414b297cc38aaa583a96"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a615af977a42e414b297cc38aaa583a96">apply_stable_softmax</a> (<a class="el" href="classTensor.html">Tensor</a> &amp;scores) const</td></tr>
<tr class="separator:a615af977a42e414b297cc38aaa583a96"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9476a40b35972bbb22c8e747685e9d7c" id="r_a9476a40b35972bbb22c8e747685e9d7c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a9476a40b35972bbb22c8e747685e9d7c">apply_rotary_embeddings</a> (<a class="el" href="classMatrix.html">Matrix</a> &amp;matrix, size_t offset=0)</td></tr>
<tr class="memdesc:a9476a40b35972bbb22c8e747685e9d7c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies rotary position embeddings to Q/K matrices.  <br /></td></tr>
<tr class="separator:a9476a40b35972bbb22c8e747685e9d7c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6cc7e11046066e5d752d9112c56d16f2" id="r_a6cc7e11046066e5d752d9112c56d16f2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a6cc7e11046066e5d752d9112c56d16f2">initialize_rope_cache</a> (size_t max_seq_len, size_t dim_idx)</td></tr>
<tr class="memdesc:a6cc7e11046066e5d752d9112c56d16f2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes or updates RoPE caches.  <br /></td></tr>
<tr class="separator:a6cc7e11046066e5d752d9112c56d16f2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a02d15d5c760c9461189debefe54dab99" id="r_a02d15d5c760c9461189debefe54dab99"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a02d15d5c760c9461189debefe54dab99">get_cos_cached</a> (size_t pos, size_t dim_idx) const</td></tr>
<tr class="separator:a02d15d5c760c9461189debefe54dab99"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad02e8c0946a244a380de6a145afbce7b" id="r_ad02e8c0946a244a380de6a145afbce7b"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ad02e8c0946a244a380de6a145afbce7b">get_sin_cached</a> (size_t pos, size_t dim_idx) const</td></tr>
<tr class="separator:ad02e8c0946a244a380de6a145afbce7b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:aaab6795c3fa57db181ffd2e9970be03b" id="r_aaab6795c3fa57db181ffd2e9970be03b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aaab6795c3fa57db181ffd2e9970be03b">params</a></td></tr>
<tr class="separator:aaab6795c3fa57db181ffd2e9970be03b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a48de8c0af70c13a90f9b7a111e7f58bf" id="r_a48de8c0af70c13a90f9b7a111e7f58bf"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a48de8c0af70c13a90f9b7a111e7f58bf">param_gradients</a></td></tr>
<tr class="separator:a48de8c0af70c13a90f9b7a111e7f58bf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a47209d4557f013d808bcc1dd8aab374d" id="r_a47209d4557f013d808bcc1dd8aab374d"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a47209d4557f013d808bcc1dd8aab374d">max_seq_length</a></td></tr>
<tr class="memdesc:a47209d4557f013d808bcc1dd8aab374d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Maximum sequence length supported.  <br /></td></tr>
<tr class="separator:a47209d4557f013d808bcc1dd8aab374d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62c5dc456e39aa00650e69315fe86af3" id="r_a62c5dc456e39aa00650e69315fe86af3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a62c5dc456e39aa00650e69315fe86af3">query_proj_grad</a></td></tr>
<tr class="separator:a62c5dc456e39aa00650e69315fe86af3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aded8e7c09f874cc1aa73f9da805a3ec6" id="r_aded8e7c09f874cc1aa73f9da805a3ec6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aded8e7c09f874cc1aa73f9da805a3ec6">key_proj_grad</a></td></tr>
<tr class="separator:aded8e7c09f874cc1aa73f9da805a3ec6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8637aa0a25d31fc7f4713f916bb1f043" id="r_a8637aa0a25d31fc7f4713f916bb1f043"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a8637aa0a25d31fc7f4713f916bb1f043">value_proj_grad</a></td></tr>
<tr class="separator:a8637aa0a25d31fc7f4713f916bb1f043"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4385a8aebf34df120d848956d8f510c5" id="r_a4385a8aebf34df120d848956d8f510c5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a4385a8aebf34df120d848956d8f510c5">output_proj_grad</a></td></tr>
<tr class="separator:a4385a8aebf34df120d848956d8f510c5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a944d2e4a686b408e94758c7458888f47" id="r_a944d2e4a686b408e94758c7458888f47"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a944d2e4a686b408e94758c7458888f47">query_bias_grad</a></td></tr>
<tr class="separator:a944d2e4a686b408e94758c7458888f47"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a872be590c370844a88aff0705d1cba72" id="r_a872be590c370844a88aff0705d1cba72"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a872be590c370844a88aff0705d1cba72">key_bias_grad</a></td></tr>
<tr class="separator:a872be590c370844a88aff0705d1cba72"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afa3b408cc8580772c3bdc32b5e25fe07" id="r_afa3b408cc8580772c3bdc32b5e25fe07"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#afa3b408cc8580772c3bdc32b5e25fe07">value_bias_grad</a></td></tr>
<tr class="separator:afa3b408cc8580772c3bdc32b5e25fe07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a891d285712cdbb98de1b453c58c23cc4" id="r_a891d285712cdbb98de1b453c58c23cc4"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a891d285712cdbb98de1b453c58c23cc4">output_bias_grad</a></td></tr>
<tr class="separator:a891d285712cdbb98de1b453c58c23cc4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18362671ef7860ee292602e8a80fdfe3" id="r_a18362671ef7860ee292602e8a80fdfe3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a18362671ef7860ee292602e8a80fdfe3">query_proj</a></td></tr>
<tr class="memdesc:a18362671ef7860ee292602e8a80fdfe3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Query projection matrix [hidden_size, num_heads * head_dim].  <br /></td></tr>
<tr class="separator:a18362671ef7860ee292602e8a80fdfe3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adccbc1658f1f13e00ea1a7cb71ec9030" id="r_adccbc1658f1f13e00ea1a7cb71ec9030"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#adccbc1658f1f13e00ea1a7cb71ec9030">key_proj</a></td></tr>
<tr class="memdesc:adccbc1658f1f13e00ea1a7cb71ec9030"><td class="mdescLeft">&#160;</td><td class="mdescRight">Key projection matrix [hidden_size, num_kv_heads * head_dim].  <br /></td></tr>
<tr class="separator:adccbc1658f1f13e00ea1a7cb71ec9030"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad0999daf1477b120755a3413685812b" id="r_aad0999daf1477b120755a3413685812b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aad0999daf1477b120755a3413685812b">value_proj</a></td></tr>
<tr class="memdesc:aad0999daf1477b120755a3413685812b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Value projection matrix [hidden_size, num_kv_heads * head_dim].  <br /></td></tr>
<tr class="separator:aad0999daf1477b120755a3413685812b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a1b86106c4351af679f4e0e2a8a0811" id="r_a4a1b86106c4351af679f4e0e2a8a0811"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a4a1b86106c4351af679f4e0e2a8a0811">output_proj</a></td></tr>
<tr class="memdesc:a4a1b86106c4351af679f4e0e2a8a0811"><td class="mdescLeft">&#160;</td><td class="mdescRight">Output projection matrix [num_heads * head_dim, hidden_size].  <br /></td></tr>
<tr class="separator:a4a1b86106c4351af679f4e0e2a8a0811"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0afc6064aba014f541fc9e2c4cf09841" id="r_a0afc6064aba014f541fc9e2c4cf09841"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a0afc6064aba014f541fc9e2c4cf09841">query_bias</a></td></tr>
<tr class="memdesc:a0afc6064aba014f541fc9e2c4cf09841"><td class="mdescLeft">&#160;</td><td class="mdescRight">Query projection bias.  <br /></td></tr>
<tr class="separator:a0afc6064aba014f541fc9e2c4cf09841"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac034c448395ee5ccb2f55f801ccaec40" id="r_ac034c448395ee5ccb2f55f801ccaec40"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ac034c448395ee5ccb2f55f801ccaec40">key_bias</a></td></tr>
<tr class="memdesc:ac034c448395ee5ccb2f55f801ccaec40"><td class="mdescLeft">&#160;</td><td class="mdescRight">Key projection bias.  <br /></td></tr>
<tr class="separator:ac034c448395ee5ccb2f55f801ccaec40"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a43b9f09cb611795b1642d239aaa502b6" id="r_a43b9f09cb611795b1642d239aaa502b6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a43b9f09cb611795b1642d239aaa502b6">value_bias</a></td></tr>
<tr class="memdesc:a43b9f09cb611795b1642d239aaa502b6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Value projection bias.  <br /></td></tr>
<tr class="separator:a43b9f09cb611795b1642d239aaa502b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e04836c7b83bb0172fd3d9a120c8239" id="r_a3e04836c7b83bb0172fd3d9a120c8239"><td class="memItemLeft" align="right" valign="top"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a3e04836c7b83bb0172fd3d9a120c8239">output_bias</a></td></tr>
<tr class="memdesc:a3e04836c7b83bb0172fd3d9a120c8239"><td class="mdescLeft">&#160;</td><td class="mdescRight">Output projection bias.  <br /></td></tr>
<tr class="separator:a3e04836c7b83bb0172fd3d9a120c8239"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34cbd06de38900793f5c452507b032fa" id="r_a34cbd06de38900793f5c452507b032fa"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a34cbd06de38900793f5c452507b032fa">num_heads</a></td></tr>
<tr class="memdesc:a34cbd06de38900793f5c452507b032fa"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of attention heads.  <br /></td></tr>
<tr class="separator:a34cbd06de38900793f5c452507b032fa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e6e670e0c4ab3ef36a100394220fd07" id="r_a1e6e670e0c4ab3ef36a100394220fd07"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a1e6e670e0c4ab3ef36a100394220fd07">head_dim</a></td></tr>
<tr class="memdesc:a1e6e670e0c4ab3ef36a100394220fd07"><td class="mdescLeft">&#160;</td><td class="mdescRight">Dimension of each attention head.  <br /></td></tr>
<tr class="separator:a1e6e670e0c4ab3ef36a100394220fd07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afa3559e2a63108418d209926f89d7f13" id="r_afa3559e2a63108418d209926f89d7f13"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#afa3559e2a63108418d209926f89d7f13">hidden_size</a></td></tr>
<tr class="memdesc:afa3559e2a63108418d209926f89d7f13"><td class="mdescLeft">&#160;</td><td class="mdescRight">Size of input and output tensors.  <br /></td></tr>
<tr class="separator:afa3559e2a63108418d209926f89d7f13"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad4b2bd9329b13739e3070253fa707abb" id="r_ad4b2bd9329b13739e3070253fa707abb"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#ad4b2bd9329b13739e3070253fa707abb">dropout_prob</a></td></tr>
<tr class="memdesc:ad4b2bd9329b13739e3070253fa707abb"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classDropout.html" title="Implements dropout regularization for neural networks.">Dropout</a> probability.  <br /></td></tr>
<tr class="separator:ad4b2bd9329b13739e3070253fa707abb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6eaf4f61461419d9b2273d8ea5b61c8c" id="r_a6eaf4f61461419d9b2273d8ea5b61c8c"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a6eaf4f61461419d9b2273d8ea5b61c8c">use_rope</a></td></tr>
<tr class="memdesc:a6eaf4f61461419d9b2273d8ea5b61c8c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether to use rotary position embeddings.  <br /></td></tr>
<tr class="separator:a6eaf4f61461419d9b2273d8ea5b61c8c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49435188dcf74478d543f7f4124bd25d" id="r_a49435188dcf74478d543f7f4124bd25d"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a49435188dcf74478d543f7f4124bd25d">use_flash</a></td></tr>
<tr class="memdesc:a49435188dcf74478d543f7f4124bd25d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether to use Flash Attention.  <br /></td></tr>
<tr class="separator:a49435188dcf74478d543f7f4124bd25d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaffc8817085cfdbcdbebf858d6ff71ba" id="r_aaffc8817085cfdbcdbebf858d6ff71ba"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aaffc8817085cfdbcdbebf858d6ff71ba">use_sliding_window</a></td></tr>
<tr class="memdesc:aaffc8817085cfdbcdbebf858d6ff71ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether to use sliding window attention.  <br /></td></tr>
<tr class="separator:aaffc8817085cfdbcdbebf858d6ff71ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a25d0ca82bde81799ff028d4f4d276806" id="r_a25d0ca82bde81799ff028d4f4d276806"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a25d0ca82bde81799ff028d4f4d276806">window_size</a></td></tr>
<tr class="memdesc:a25d0ca82bde81799ff028d4f4d276806"><td class="mdescLeft">&#160;</td><td class="mdescRight">Size of attention window.  <br /></td></tr>
<tr class="separator:a25d0ca82bde81799ff028d4f4d276806"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a58478592fe138677a9a9b923460dfaef" id="r_a58478592fe138677a9a9b923460dfaef"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a58478592fe138677a9a9b923460dfaef">use_gqa</a></td></tr>
<tr class="memdesc:a58478592fe138677a9a9b923460dfaef"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether to use grouped query attention.  <br /></td></tr>
<tr class="separator:a58478592fe138677a9a9b923460dfaef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5b61a3ff6c0d3debbf2c73e3a110aecc" id="r_a5b61a3ff6c0d3debbf2c73e3a110aecc"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a5b61a3ff6c0d3debbf2c73e3a110aecc">num_kv_heads</a></td></tr>
<tr class="memdesc:a5b61a3ff6c0d3debbf2c73e3a110aecc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of key/value heads for GQA.  <br /></td></tr>
<tr class="separator:a5b61a3ff6c0d3debbf2c73e3a110aecc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5549a32f6e8ee18e5e69d7a56f1365de" id="r_a5549a32f6e8ee18e5e69d7a56f1365de"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a5549a32f6e8ee18e5e69d7a56f1365de">cos_cached</a></td></tr>
<tr class="memdesc:a5549a32f6e8ee18e5e69d7a56f1365de"><td class="mdescLeft">&#160;</td><td class="mdescRight">Cached cosine values for RoPE.  <br /></td></tr>
<tr class="separator:a5549a32f6e8ee18e5e69d7a56f1365de"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef4835723fe043b288a6ac6af6299159" id="r_aef4835723fe043b288a6ac6af6299159"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMatrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#aef4835723fe043b288a6ac6af6299159">sin_cached</a></td></tr>
<tr class="memdesc:aef4835723fe043b288a6ac6af6299159"><td class="mdescLeft">&#160;</td><td class="mdescRight">Cached sine values for RoPE.  <br /></td></tr>
<tr class="separator:aef4835723fe043b288a6ac6af6299159"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4314826233aeaa0ba472897dbb1ee7e9" id="r_a4314826233aeaa0ba472897dbb1ee7e9"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#a4314826233aeaa0ba472897dbb1ee7e9">use_fp16_</a></td></tr>
<tr class="separator:a4314826233aeaa0ba472897dbb1ee7e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="friends" name="friends"></a>
Friends</h2></td></tr>
<tr class="memitem:afda6f8e578113ef943a35d8f29af143a" id="r_afda6f8e578113ef943a35d8f29af143a"><td class="memItemLeft" align="right" valign="top">class&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMultiHeadAttention.html#afda6f8e578113ef943a35d8f29af143a">Transformer</a></td></tr>
<tr class="separator:afda6f8e578113ef943a35d8f29af143a"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Implementation of Multi-Head Attention mechanism with various optimizations. </p>
<p>This class implements the core attention mechanism used in transformers, with support for:</p><ul>
<li>Multi-head attention with separate Q/K/V projections</li>
<li>Grouped Query Attention (GQA)</li>
<li>Rotary Position Embeddings (RoPE)</li>
<li>Flash Attention optimization</li>
<li>Sliding window attention</li>
<li>Key-Value caching for efficient inference </li>
</ul>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00049">49</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a8e0b7839939a2076d6c92a8a59443364" name="a8e0b7839939a2076d6c92a8a59443364"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8e0b7839939a2076d6c92a8a59443364">&#9670;&#160;</a></span>~MultiHeadAttention()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual MultiHeadAttention::~MultiHeadAttention </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span><span class="mlabel">default</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="afd7c283ed2457ad7304567c897b3d877" name="afd7c283ed2457ad7304567c897b3d877"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afd7c283ed2457ad7304567c897b3d877">&#9670;&#160;</a></span>MultiHeadAttention() <span class="overload">[1/3]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">MultiHeadAttention::MultiHeadAttention </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">default</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a5bb39f2850f5824fd23df006b4693299" name="a5bb39f2850f5824fd23df006b4693299"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5bb39f2850f5824fd23df006b4693299">&#9670;&#160;</a></span>MultiHeadAttention() <span class="overload">[2/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">MultiHeadAttention::MultiHeadAttention </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>hidden_size_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>num_heads_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>head_dim_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>dropout_prob_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_flash_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_rope_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_sliding_window_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>window_size_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_gqa_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>num_kv_heads_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>max_seq_length_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_fp16</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Constructs a multi-head attention layer with the specified parameters. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">hidden_size_</td><td>Size of the input and output tensors </td></tr>
    <tr><td class="paramname">num_heads_</td><td>Number of attention heads </td></tr>
    <tr><td class="paramname">head_dim_</td><td>Dimension of each attention head </td></tr>
    <tr><td class="paramname">dropout_prob_</td><td><a class="el" href="classDropout.html" title="Implements dropout regularization for neural networks.">Dropout</a> probability </td></tr>
    <tr><td class="paramname">use_flash_</td><td>Whether to use Flash Attention optimization </td></tr>
    <tr><td class="paramname">use_rope_</td><td>Whether to use rotary position embeddings </td></tr>
    <tr><td class="paramname">use_sliding_window_</td><td>Whether to use sliding window attention </td></tr>
    <tr><td class="paramname">window_size_</td><td>Size of the sliding window </td></tr>
    <tr><td class="paramname">use_gqa_</td><td>Whether to use grouped query attention </td></tr>
    <tr><td class="paramname">num_kv_heads_</td><td>Number of key/value heads for GQA </td></tr>
    <tr><td class="paramname">max_seq_length_</td><td>Maximum sequence length supported </td></tr>
    <tr><td class="paramname">use_fp16_</td><td>Whether to use fp16 for computation </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00442">442</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a5c83c47bf2bf22760110edeb4375ac8e" name="a5c83c47bf2bf22760110edeb4375ac8e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5c83c47bf2bf22760110edeb4375ac8e">&#9670;&#160;</a></span>MultiHeadAttention() <span class="overload">[3/3]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">MultiHeadAttention::MultiHeadAttention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &amp;&#160;</td>
          <td class="paramname"><em>other</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00140">140</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a3ccbdfe184867bfb5ba90cd103bd2eb0" name="a3ccbdfe184867bfb5ba90cd103bd2eb0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3ccbdfe184867bfb5ba90cd103bd2eb0">&#9670;&#160;</a></span>apply_mask() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::apply_mask </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>scores</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00370">370</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a3bb386388f773c8b72eb2af62c66219c" name="a3bb386388f773c8b72eb2af62c66219c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3bb386388f773c8b72eb2af62c66219c">&#9670;&#160;</a></span>apply_mask() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::apply_mask </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classTensor.html">Tensor</a> &amp;&#160;</td>
          <td class="paramname"><em>scores</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00393">393</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a15a9a52d7adb5b6c4fe5cddb273acff6" name="a15a9a52d7adb5b6c4fe5cddb273acff6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a15a9a52d7adb5b6c4fe5cddb273acff6">&#9670;&#160;</a></span>apply_rope()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classVector.html">Vector</a> MultiHeadAttention::apply_rope </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classVector.html">Vector</a> &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>position</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00016">16</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a9476a40b35972bbb22c8e747685e9d7c" name="a9476a40b35972bbb22c8e747685e9d7c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9476a40b35972bbb22c8e747685e9d7c">&#9670;&#160;</a></span>apply_rotary_embeddings()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::apply_rotary_embeddings </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>matrix</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>offset</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Applies rotary position embeddings to Q/K matrices. </p>
<p>RoPE improves the model's ability to capture positional information by encoding positions through rotation in vector space.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">matrix</td><td>Input Q or K matrix to apply RoPE to </td></tr>
    <tr><td class="paramname">offset</td><td>Position offset for cached generation </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a666bd13dfa5833931c2b3176ebf3d2f8" name="a666bd13dfa5833931c2b3176ebf3d2f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a666bd13dfa5833931c2b3176ebf3d2f8">&#9670;&#160;</a></span>apply_stable_softmax() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::apply_stable_softmax </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l01077">1077</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a615af977a42e414b297cc38aaa583a96" name="a615af977a42e414b297cc38aaa583a96"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a615af977a42e414b297cc38aaa583a96">&#9670;&#160;</a></span>apply_stable_softmax() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::apply_stable_softmax </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classTensor.html">Tensor</a> &amp;&#160;</td>
          <td class="paramname"><em>scores</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00400">400</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a0d5c79966001a4f69b95c8fdcad85e2b" name="a0d5c79966001a4f69b95c8fdcad85e2b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0d5c79966001a4f69b95c8fdcad85e2b">&#9670;&#160;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::backward </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>grad_output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>target_distribution</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Performs the backward pass to compute gradients. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">grad_output</td><td>Gradient of the loss with respect to the output </td></tr>
    <tr><td class="paramname">input</td><td>Original input tensor </td></tr>
    <tr><td class="paramname">target_distribution</td><td>Target attention distribution (for distillation) </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Gradient with respect to the input </dd></dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00663">663</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a294080c43c3b337d7525e7b7e380d89c" name="a294080c43c3b337d7525e7b7e380d89c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a294080c43c3b337d7525e7b7e380d89c">&#9670;&#160;</a></span>backward_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::backward_cuda </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>grad</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>input</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>CUDA-accelerated version of the backward pass. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">grad</td><td>Gradient of the loss with respect to the output </td></tr>
    <tr><td class="paramname">input</td><td>Original input tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Gradient with respect to the input </dd></dl>

</div>
</div>
<a id="a6e2075c78c6a20f11ea661d31ecffa32" name="a6e2075c78c6a20f11ea661d31ecffa32"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6e2075c78c6a20f11ea661d31ecffa32">&#9670;&#160;</a></span>combine_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::combine_gradients </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>dQ</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>dK</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>dV</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00349">349</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="ab19f508223deaa017a7f924f0946d25a" name="ab19f508223deaa017a7f924f0946d25a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab19f508223deaa017a7f924f0946d25a">&#9670;&#160;</a></span>compute_attention() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::compute_attention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>Q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>K</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>V</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00781">781</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a0df29ad720ff58e8ffd73d3fcb2e6b26" name="a0df29ad720ff58e8ffd73d3fcb2e6b26"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0df29ad720ff58e8ffd73d3fcb2e6b26">&#9670;&#160;</a></span>compute_attention() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a> MultiHeadAttention::compute_attention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>Q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>K</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>V</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batch_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>seq_len</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>head_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00878">878</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a36b374cd10b12a404825b10ca5371096" name="a36b374cd10b12a404825b10ca5371096"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a36b374cd10b12a404825b10ca5371096">&#9670;&#160;</a></span>compute_attention_scores()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::compute_attention_scores </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>Q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>K</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Computes attention scores between query and key matrices. </p>
<p>Implements the scaled dot-product attention mechanism: scores = softmax(Q * K^T / sqrt(head_dim))</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">Q</td><td>Query matrix [batch_size * num_heads, seq_len, head_dim] </td></tr>
    <tr><td class="paramname">K</td><td>Key matrix [batch_size * num_kv_heads, seq_len, head_dim] </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Attention scores [batch_size * num_heads, seq_len, seq_len] </dd></dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l01039">1039</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a2af0984f2e552f9bbad0eb9a4f33e1dc" name="a2af0984f2e552f9bbad0eb9a4f33e1dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2af0984f2e552f9bbad0eb9a4f33e1dc">&#9670;&#160;</a></span>compute_key_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::compute_key_gradients </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>grad_output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>input</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00337">337</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="aa153e5d1204e9e40db741b979b8da86d" name="aa153e5d1204e9e40db741b979b8da86d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa153e5d1204e9e40db741b979b8da86d">&#9670;&#160;</a></span>compute_query_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::compute_query_gradients </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>grad_output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>input</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00331">331</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="afb26ebaa99dac3ec111eeeb60baf96cf" name="afb26ebaa99dac3ec111eeeb60baf96cf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb26ebaa99dac3ec111eeeb60baf96cf">&#9670;&#160;</a></span>compute_value_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::compute_value_gradients </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>grad_output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>input</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00343">343</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a2ae83b92dbfb2da91f2a158d17c701ff" name="a2ae83b92dbfb2da91f2a158d17c701ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ae83b92dbfb2da91f2a158d17c701ff">&#9670;&#160;</a></span>flash_attention()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::flash_attention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>Q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>K</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>V</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00053">53</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a9c5b1b6ba053f3891ddde42588fb8e7e" name="a9c5b1b6ba053f3891ddde42588fb8e7e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9c5b1b6ba053f3891ddde42588fb8e7e">&#9670;&#160;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::forward </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::optional&lt; <a class="el" href="structKVCache.html">KVCache</a> &gt; &amp;&#160;</td>
          <td class="paramname"><em>kv_cache</em> = <code>std::nullopt</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Performs the forward pass of the attention mechanism. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>Input tensor of shape [batch_size, seq_len, hidden_size] </td></tr>
    <tr><td class="paramname">mask</td><td>Attention mask to prevent attending to certain positions </td></tr>
    <tr><td class="paramname">kv_cache</td><td>Optional cache of key and value projections for efficient inference </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Output tensor of shape [batch_size, seq_len, hidden_size] </dd></dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00139">139</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a02d15d5c760c9461189debefe54dab99" name="a02d15d5c760c9461189debefe54dab99"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a02d15d5c760c9461189debefe54dab99">&#9670;&#160;</a></span>get_cos_cached()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">float MultiHeadAttention::get_cos_cached </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>pos</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>dim_idx</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l01023">1023</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="ad02e8c0946a244a380de6a145afbce7b" name="ad02e8c0946a244a380de6a145afbce7b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad02e8c0946a244a380de6a145afbce7b">&#9670;&#160;</a></span>get_sin_cached()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">float MultiHeadAttention::get_sin_cached </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>pos</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>dim_idx</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l01031">1031</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a8c6f562cadc8d96c6d813903ab68eba0" name="a8c6f562cadc8d96c6d813903ab68eba0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8c6f562cadc8d96c6d813903ab68eba0">&#9670;&#160;</a></span>get_weights()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; std::reference_wrapper&lt; <a class="el" href="classMatrix.html">Matrix</a> &gt; &gt; MultiHeadAttention::get_weights </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets references to all trainable weight matrices. </p>
<dl class="section return"><dt>Returns</dt><dd><a class="el" href="classVector.html">Vector</a> of references to weight matrices </dd></dl>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00120">120</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a8f6731eea02865fe7875c2bc51f8aaab" name="a8f6731eea02865fe7875c2bc51f8aaab"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8f6731eea02865fe7875c2bc51f8aaab">&#9670;&#160;</a></span>getKeyBias()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp; MultiHeadAttention::getKeyBias </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00130">130</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="ab8ac3041965fd7cd55c5c6baddedec10" name="ab8ac3041965fd7cd55c5c6baddedec10"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab8ac3041965fd7cd55c5c6baddedec10">&#9670;&#160;</a></span>getOutputBias()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp; MultiHeadAttention::getOutputBias </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00136">136</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="acc1115481c87ae82ef5f5dc1240abd6c" name="acc1115481c87ae82ef5f5dc1240abd6c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acc1115481c87ae82ef5f5dc1240abd6c">&#9670;&#160;</a></span>getQueryBias()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp; MultiHeadAttention::getQueryBias </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00127">127</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="ac88561cc62d2f36f06ca0425a379d635" name="ac88561cc62d2f36f06ca0425a379d635"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac88561cc62d2f36f06ca0425a379d635">&#9670;&#160;</a></span>getValueBias()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> &amp; MultiHeadAttention::getValueBias </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00133">133</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a6cc7e11046066e5d752d9112c56d16f2" name="a6cc7e11046066e5d752d9112c56d16f2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6cc7e11046066e5d752d9112c56d16f2">&#9670;&#160;</a></span>initialize_rope_cache()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::initialize_rope_cache </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>max_seq_len</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>dim_idx</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Initializes or updates RoPE caches. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">max_seq_len</td><td>Length of sequence to cache embeddings for </td></tr>
    <tr><td class="paramname">dim_idx</td><td>Dimension index for cached generation </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00998">998</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="acdb722e2273e30fee53d5dc11d60e956" name="acdb722e2273e30fee53d5dc11d60e956"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acdb722e2273e30fee53d5dc11d60e956">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::unique_ptr&lt; <a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &gt; MultiHeadAttention::load </td>
          <td>(</td>
          <td class="paramtype">std::istream &amp;&#160;</td>
          <td class="paramname"><em>is</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classTransformerConfig.html">TransformerConfig</a> &amp;&#160;</td>
          <td class="paramname"><em>config</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Loads attention layer parameters from a stream. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">is</td><td>Input stream to load from </td></tr>
    <tr><td class="paramname">config</td><td>Configuration object </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Unique pointer to the loaded attention layer </dd></dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00391">391</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a1af65b07493c526796f14b9d7599f364" name="a1af65b07493c526796f14b9d7599f364"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1af65b07493c526796f14b9d7599f364">&#9670;&#160;</a></span>operator=()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &amp; MultiHeadAttention::operator= </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMultiHeadAttention.html">MultiHeadAttention</a> &amp;&#160;</td>
          <td class="paramname"><em>other</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00154">154</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a91beae69ea948d75a451e65b568ed13d" name="a91beae69ea948d75a451e65b568ed13d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a91beae69ea948d75a451e65b568ed13d">&#9670;&#160;</a></span>parameter_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a> &amp; MultiHeadAttention::parameter_gradients </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00226">226</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a19414c0bde6474025e97ab357da7819e" name="a19414c0bde6474025e97ab357da7819e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a19414c0bde6474025e97ab357da7819e">&#9670;&#160;</a></span>parameters()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a> &amp; MultiHeadAttention::parameters </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00207">207</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="ad58475dba6a9acf0f788a069807ca678" name="ad58475dba6a9acf0f788a069807ca678"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad58475dba6a9acf0f788a069807ca678">&#9670;&#160;</a></span>reshape_for_attention()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a> MultiHeadAttention::reshape_for_attention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batch_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>seq_len</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>head_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00725">725</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="aff569533c7e7f11a7fa4b6ba1662efc5" name="aff569533c7e7f11a7fa4b6ba1662efc5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff569533c7e7f11a7fa4b6ba1662efc5">&#9670;&#160;</a></span>reshape_from_attention()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::reshape_from_attention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a> &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>seq_len</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>hidden_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00746">746</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="ad2acdc3607094de7bd5e121432084947" name="ad2acdc3607094de7bd5e121432084947"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad2acdc3607094de7bd5e121432084947">&#9670;&#160;</a></span>safe_matmul()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::safe_matmul </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>B</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00362">362</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a0386f6537c985151921e37a2008d6d59" name="a0386f6537c985151921e37a2008d6d59"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0386f6537c985151921e37a2008d6d59">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::save </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;&#160;</td>
          <td class="paramname"><em>os</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Saves the attention layer parameters to a stream. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>Output stream to save to </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00364">364</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="aa63f2eb55c928afcbaabe2bed9b5fe9f" name="aa63f2eb55c928afcbaabe2bed9b5fe9f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa63f2eb55c928afcbaabe2bed9b5fe9f">&#9670;&#160;</a></span>standard_attention()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::standard_attention </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>Q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>K</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>V</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classAttentionMask.html">AttentionMask</a> &amp;&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8cpp_source.html#l00566">566</a> of file <a class="el" href="attention_8cpp_source.html">attention.cpp</a>.</p>

</div>
</div>
<a id="a595e7485ec3665e5dae0b13e820a529b" name="a595e7485ec3665e5dae0b13e820a529b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a595e7485ec3665e5dae0b13e820a529b">&#9670;&#160;</a></span>validate_dimensions()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void MultiHeadAttention::validate_dimensions </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>grad_output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classMatrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>target_dist</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00318">318</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Friends And Related Symbol Documentation</h2>
<a id="afda6f8e578113ef943a35d8f29af143a" name="afda6f8e578113ef943a35d8f29af143a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afda6f8e578113ef943a35d8f29af143a">&#9670;&#160;</a></span>Transformer</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">friend class <a class="el" href="classTransformer.html">Transformer</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">friend</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00125">125</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a5549a32f6e8ee18e5e69d7a56f1365de" name="a5549a32f6e8ee18e5e69d7a56f1365de"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5549a32f6e8ee18e5e69d7a56f1365de">&#9670;&#160;</a></span>cos_cached</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::cos_cached</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Cached cosine values for RoPE. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00299">299</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="ad4b2bd9329b13739e3070253fa707abb" name="ad4b2bd9329b13739e3070253fa707abb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad4b2bd9329b13739e3070253fa707abb">&#9670;&#160;</a></span>dropout_prob</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">float MultiHeadAttention::dropout_prob</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p><a class="el" href="classDropout.html" title="Implements dropout regularization for neural networks.">Dropout</a> probability. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00290">290</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a1e6e670e0c4ab3ef36a100394220fd07" name="a1e6e670e0c4ab3ef36a100394220fd07"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e6e670e0c4ab3ef36a100394220fd07">&#9670;&#160;</a></span>head_dim</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t MultiHeadAttention::head_dim</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Dimension of each attention head. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00288">288</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="afa3559e2a63108418d209926f89d7f13" name="afa3559e2a63108418d209926f89d7f13"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afa3559e2a63108418d209926f89d7f13">&#9670;&#160;</a></span>hidden_size</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t MultiHeadAttention::hidden_size</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Size of input and output tensors. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00289">289</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="ac034c448395ee5ccb2f55f801ccaec40" name="ac034c448395ee5ccb2f55f801ccaec40"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac034c448395ee5ccb2f55f801ccaec40">&#9670;&#160;</a></span>key_bias</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::key_bias</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Key projection bias. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00282">282</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a872be590c370844a88aff0705d1cba72" name="a872be590c370844a88aff0705d1cba72"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a872be590c370844a88aff0705d1cba72">&#9670;&#160;</a></span>key_bias_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::key_bias_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00270">270</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="adccbc1658f1f13e00ea1a7cb71ec9030" name="adccbc1658f1f13e00ea1a7cb71ec9030"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adccbc1658f1f13e00ea1a7cb71ec9030">&#9670;&#160;</a></span>key_proj</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::key_proj</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Key projection matrix [hidden_size, num_kv_heads * head_dim]. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00276">276</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="aded8e7c09f874cc1aa73f9da805a3ec6" name="aded8e7c09f874cc1aa73f9da805a3ec6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aded8e7c09f874cc1aa73f9da805a3ec6">&#9670;&#160;</a></span>key_proj_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::key_proj_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00266">266</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a47209d4557f013d808bcc1dd8aab374d" name="a47209d4557f013d808bcc1dd8aab374d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a47209d4557f013d808bcc1dd8aab374d">&#9670;&#160;</a></span>max_seq_length</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t MultiHeadAttention::max_seq_length</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Maximum sequence length supported. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00262">262</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a34cbd06de38900793f5c452507b032fa" name="a34cbd06de38900793f5c452507b032fa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a34cbd06de38900793f5c452507b032fa">&#9670;&#160;</a></span>num_heads</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t MultiHeadAttention::num_heads</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Number of attention heads. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00287">287</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a5b61a3ff6c0d3debbf2c73e3a110aecc" name="a5b61a3ff6c0d3debbf2c73e3a110aecc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5b61a3ff6c0d3debbf2c73e3a110aecc">&#9670;&#160;</a></span>num_kv_heads</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t MultiHeadAttention::num_kv_heads</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Number of key/value heads for GQA. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00296">296</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a3e04836c7b83bb0172fd3d9a120c8239" name="a3e04836c7b83bb0172fd3d9a120c8239"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e04836c7b83bb0172fd3d9a120c8239">&#9670;&#160;</a></span>output_bias</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::output_bias</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Output projection bias. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00284">284</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a891d285712cdbb98de1b453c58c23cc4" name="a891d285712cdbb98de1b453c58c23cc4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a891d285712cdbb98de1b453c58c23cc4">&#9670;&#160;</a></span>output_bias_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::output_bias_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00272">272</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a4a1b86106c4351af679f4e0e2a8a0811" name="a4a1b86106c4351af679f4e0e2a8a0811"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a1b86106c4351af679f4e0e2a8a0811">&#9670;&#160;</a></span>output_proj</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::output_proj</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Output projection matrix [num_heads * head_dim, hidden_size]. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00278">278</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a4385a8aebf34df120d848956d8f510c5" name="a4385a8aebf34df120d848956d8f510c5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4385a8aebf34df120d848956d8f510c5">&#9670;&#160;</a></span>output_proj_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::output_proj_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00268">268</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a48de8c0af70c13a90f9b7a111e7f58bf" name="a48de8c0af70c13a90f9b7a111e7f58bf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a48de8c0af70c13a90f9b7a111e7f58bf">&#9670;&#160;</a></span>param_gradients</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a> MultiHeadAttention::param_gradients</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00259">259</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="aaab6795c3fa57db181ffd2e9970be03b" name="aaab6795c3fa57db181ffd2e9970be03b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaab6795c3fa57db181ffd2e9970be03b">&#9670;&#160;</a></span>params</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structMultiHeadAttention_1_1Parameters.html">Parameters</a> MultiHeadAttention::params</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00258">258</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a0afc6064aba014f541fc9e2c4cf09841" name="a0afc6064aba014f541fc9e2c4cf09841"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0afc6064aba014f541fc9e2c4cf09841">&#9670;&#160;</a></span>query_bias</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::query_bias</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Query projection bias. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00281">281</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a944d2e4a686b408e94758c7458888f47" name="a944d2e4a686b408e94758c7458888f47"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a944d2e4a686b408e94758c7458888f47">&#9670;&#160;</a></span>query_bias_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::query_bias_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00269">269</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a18362671ef7860ee292602e8a80fdfe3" name="a18362671ef7860ee292602e8a80fdfe3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18362671ef7860ee292602e8a80fdfe3">&#9670;&#160;</a></span>query_proj</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::query_proj</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Query projection matrix [hidden_size, num_heads * head_dim]. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00275">275</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a62c5dc456e39aa00650e69315fe86af3" name="a62c5dc456e39aa00650e69315fe86af3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a62c5dc456e39aa00650e69315fe86af3">&#9670;&#160;</a></span>query_proj_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::query_proj_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00265">265</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="aef4835723fe043b288a6ac6af6299159" name="aef4835723fe043b288a6ac6af6299159"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef4835723fe043b288a6ac6af6299159">&#9670;&#160;</a></span>sin_cached</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::sin_cached</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Cached sine values for RoPE. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00300">300</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a49435188dcf74478d543f7f4124bd25d" name="a49435188dcf74478d543f7f4124bd25d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a49435188dcf74478d543f7f4124bd25d">&#9670;&#160;</a></span>use_flash</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool MultiHeadAttention::use_flash</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Whether to use Flash Attention. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00292">292</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a4314826233aeaa0ba472897dbb1ee7e9" name="a4314826233aeaa0ba472897dbb1ee7e9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4314826233aeaa0ba472897dbb1ee7e9">&#9670;&#160;</a></span>use_fp16_</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool MultiHeadAttention::use_fp16_</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00429">429</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a58478592fe138677a9a9b923460dfaef" name="a58478592fe138677a9a9b923460dfaef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a58478592fe138677a9a9b923460dfaef">&#9670;&#160;</a></span>use_gqa</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool MultiHeadAttention::use_gqa</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Whether to use grouped query attention. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00295">295</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a6eaf4f61461419d9b2273d8ea5b61c8c" name="a6eaf4f61461419d9b2273d8ea5b61c8c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6eaf4f61461419d9b2273d8ea5b61c8c">&#9670;&#160;</a></span>use_rope</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool MultiHeadAttention::use_rope</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Whether to use rotary position embeddings. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00291">291</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="aaffc8817085cfdbcdbebf858d6ff71ba" name="aaffc8817085cfdbcdbebf858d6ff71ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaffc8817085cfdbcdbebf858d6ff71ba">&#9670;&#160;</a></span>use_sliding_window</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool MultiHeadAttention::use_sliding_window</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Whether to use sliding window attention. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00293">293</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a43b9f09cb611795b1642d239aaa502b6" name="a43b9f09cb611795b1642d239aaa502b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a43b9f09cb611795b1642d239aaa502b6">&#9670;&#160;</a></span>value_bias</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::value_bias</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Value projection bias. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00283">283</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="afa3b408cc8580772c3bdc32b5e25fe07" name="afa3b408cc8580772c3bdc32b5e25fe07"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afa3b408cc8580772c3bdc32b5e25fe07">&#9670;&#160;</a></span>value_bias_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="attention_8hpp.html#af508d216e3129f80e30ae4105caa6b9d">FloatVector</a> MultiHeadAttention::value_bias_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00271">271</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="aad0999daf1477b120755a3413685812b" name="aad0999daf1477b120755a3413685812b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad0999daf1477b120755a3413685812b">&#9670;&#160;</a></span>value_proj</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::value_proj</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Value projection matrix [hidden_size, num_kv_heads * head_dim]. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00277">277</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a8637aa0a25d31fc7f4713f916bb1f043" name="a8637aa0a25d31fc7f4713f916bb1f043"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8637aa0a25d31fc7f4713f916bb1f043">&#9670;&#160;</a></span>value_proj_grad</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMatrix.html">Matrix</a> MultiHeadAttention::value_proj_grad</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">mutable</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00267">267</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<a id="a25d0ca82bde81799ff028d4f4d276806" name="a25d0ca82bde81799ff028d4f4d276806"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a25d0ca82bde81799ff028d4f4d276806">&#9670;&#160;</a></span>window_size</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t MultiHeadAttention::window_size</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Size of attention window. </p>

<p class="definition">Definition at line <a class="el" href="attention_8hpp_source.html#l00294">294</a> of file <a class="el" href="attention_8hpp_source.html">attention.hpp</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>include/<a class="el" href="attention_8hpp_source.html">attention.hpp</a></li>
<li>src/<a class="el" href="attention_8cpp_source.html">attention.cpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
